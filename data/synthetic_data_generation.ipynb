{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00297ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "np.random.seed(2020)\n",
    "torch.manual_seed(2020)\n",
    "torch.cuda.manual_seed(2020)\n",
    "torch.cuda.manual_seed_all(2020)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3474a616",
   "metadata": {},
   "source": [
    "# Generate synthetic data using a line graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfb8bf3",
   "metadata": {},
   "source": [
    "Here, we generate the synthetic data using a line graph which consists of $N+2$ points (with additional $2$ endpoints since we are using a neighborhood size of $2$). The treatments $T$, covariates $\\boldsymbol{X}$, and unobserved confounders $U$ are generated as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "\\boldsymbol{X}_i &\\sim \\mathcal{N}(\\boldsymbol{0}, \\sigma_{X}^2 \\boldsymbol{I}) \\\\\n",
    "\\{U\\}_{i=1}^{N} &\\sim \\mathcal{N}_{N}(\\boldsymbol{0}_{N}, \\boldsymbol{D}_N) \\\\\n",
    "T_i | \\boldsymbol{X}_i, U_i &\\sim f_{\\theta} (\\boldsymbol{X}_i, U_i) > 0 \\\\\n",
    "Y_i | \\boldsymbol{T}_i, \\boldsymbol{X}_i, U_i &\\sim \\beta T_i + g_{\\theta_T}(d_i \\odot \\boldsymbol{T}_{-i}) + g_{\\theta_X}(\\boldsymbol{X}_i) + U_i + \\epsilon\n",
    "\\end{align*}\n",
    "\n",
    "where $\\boldsymbol{0}_{N}$ is a zero vector of length $N$, $\\boldsymbol{D}_N$ is an $N \\times N$ matrix with $\\boldsymbol{D}_{ij} = \\frac{1}{\\sigma_U \\sqrt{2\\pi}} \\exp‚Å°\\left(-\\frac{d_{ij}}{2l^2}\\right)$, and $d_i$ is the $i^{th}$ row of $\\boldsymbol{D}_N$. $T_i$ must be greater than zero to satisfy the positivity assumption. $\\boldsymbol{T}_{-i}$ represents the neighboring treatment assignments of the unit $i$ (excluding unit $i$) depending on the neighborhood size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ed8ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500\n",
    "x_dim = 4\n",
    "sigma_x, lengthscale_u, sigma_u = 1., 0.5, 0.5\n",
    "noise_scale = 0.1\n",
    "\n",
    "# Generate a total of N + 2 samples to account for endpoints\n",
    "s1, s2 = np.linspace(0, 1, N+2), np.linspace(0, 1, N+2)\n",
    "s_grid = np.meshgrid(s1, s2)\n",
    "D_n = np.exp(-np.abs(s_grid[1]-s_grid[0])/(2*lengthscale_u**2)) / (np.sqrt(2*np.pi)*sigma_u)\n",
    "X = np.random.multivariate_normal(np.zeros(x_dim), np.eye(x_dim)*sigma_x**2, size=N+2)\n",
    "U = np.random.multivariate_normal(np.zeros(N+2), D_n, size=1).T\n",
    "\n",
    "# Use nonlinear mapping to generate T given X, U\n",
    "hidden_dim_f = 10\n",
    "f = nn.Sequential(\n",
    "    nn.Linear(x_dim+1, hidden_dim_f), \n",
    "    nn.LeakyReLU(), \n",
    "    nn.Linear(hidden_dim_f, 1), \n",
    "    nn.Softplus()\n",
    ")\n",
    "with torch.no_grad():\n",
    "    T = f(torch.tensor(np.concatenate([X,U],axis=1)).float()).numpy()\n",
    "\n",
    "# Use nonlinear mapping to generate Y given T, T_bar, X, U\n",
    "neighborhood_size = 1\n",
    "hidden_dim_g_T, hidden_dim_g_X = 16, 16\n",
    "g_T = nn.Sequential(\n",
    "    nn.Linear(2*neighborhood_size, hidden_dim_g_T), \n",
    "    nn.LeakyReLU(), \n",
    "    nn.Linear(hidden_dim_g_T, hidden_dim_g_T), \n",
    "    nn.LeakyReLU(), \n",
    "    nn.Linear(hidden_dim_g_T, 1)\n",
    ")\n",
    "g_X = nn.Sequential(\n",
    "    nn.Linear(x_dim, hidden_dim_g_X), \n",
    "    nn.LeakyReLU(), \n",
    "    nn.Linear(hidden_dim_g_X, hidden_dim_g_X), \n",
    "    nn.LeakyReLU(), \n",
    "    nn.Linear(hidden_dim_g_X, 1)\n",
    ")\n",
    "# Shape of T_bar is (N, neighborhood_size*2)\n",
    "T_bar_left = np.array([D_n[i+1,i+1-neighborhood_size:i+1]*T[i+1-neighborhood_size:i+1,0] for i in range(N)])\n",
    "T_bar_right = np.array([D_n[i+1,i+2:i+2+neighborhood_size]*T[i+2:i+2+neighborhood_size,0] for i in range(N)])\n",
    "T_bar = np.concatenate([T_bar_left,T_bar_right], axis=1)\n",
    "W = np.array([D_n[i+1,i+1-neighborhood_size:i+2+neighborhood_size] for i in range(N)])\n",
    "with torch.no_grad():\n",
    "    Y_t_bar = g_T(torch.tensor(T_bar).float()).numpy()\n",
    "    Y_t_bar_0 = g_T(torch.zeros_like(torch.tensor(T_bar)).float()).numpy()\n",
    "    Y_x = g_X(torch.tensor(X).float()).numpy()\n",
    "# Truncate T, X, and U to the same size as T_bar\n",
    "T, X, U, Y_x = T[1:N+1], X[1:N+1], U[1:N+1], Y_x[1:N+1]\n",
    "# Generate Y\n",
    "beta = np.random.rand()\n",
    "noise = np.random.normal(loc=0.0, scale=noise_scale, size=N).reshape(-1,1)\n",
    "Y_00 = Y_t_bar_0 + Y_x + U + noise   \n",
    "Y_01 = Y_t_bar + Y_x + U + noise\n",
    "Y_10 = beta*T + Y_t_bar_0 + Y_x + U + noise\n",
    "Y_11 = beta*T + Y_t_bar + Y_x + U + noise\n",
    "\n",
    "# Calculate ground truth of direct and indirect effects\n",
    "de_0, de_1 = np.mean(Y_10 - Y_00), np.mean(Y_11 - Y_01)\n",
    "ie_0, ie_1 = np.mean(Y_01 - Y_00), np.mean(Y_11 - Y_10)\n",
    "te = np.mean(Y_11 - Y_00)\n",
    "\n",
    "data = {\n",
    "    \"neighborhood_size\": neighborhood_size, \n",
    "    \"T\": T, \"T_bar\": T_bar, \"X\": X, \"Y\": Y_11, \"W\": W, \"s\": s1[1:-1, np.newaxis], \n",
    "    \"de_0\": de_0, \"de_1\": de_1, \"ie_0\": ie_0, \"ie_1\": ie_1, \"te\": te, \n",
    "    \"Y_t\": beta*T, \"Y_t_bar\": Y_t_bar, \"Y_x\": Y_x, \"U\": U\n",
    "}\n",
    "with open(\"./synthetic_data.pkl\", \"wb\") as fp:\n",
    "    pkl.dump(data, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
